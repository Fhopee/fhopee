

import pandas as pd
import json
import os
import requests
import time
from openai import OpenAI
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm  # è¿›åº¦æ¡åº“
import threading
import re
import glob

# ================= ğŸ”§ é…ç½®åŒºåŸŸ =================
DEEPSEEK_API_KEY = "sk-9d2f0895548b49eaaf57543866af62e2" # ä½ çš„ Key
OLD_WEBSITE_DOMAIN = "http://www.fhopepack.com" # ä½ çš„æ—§åŸŸå (ä¸è¦å¸¦æœ«å°¾æ–œæ )
TARGET_LANGUAGE = "English"
# ==============================================

# å¹¶å‘æ•°é‡ (å»ºè®®è®¾ç½® 3-5ã€‚å¤ªé«˜ä¼šå¯¼è‡´ DeepSeek æŠ¥é”™ 429 Too Many Requests)
MAX_WORKERS = 5 
# ==============================================

client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url="https://api.deepseek.com")

# åˆ›å»ºä¸€ä¸ªé”ï¼Œé˜²æ­¢å¤šä¸ªçº¿ç¨‹åŒæ—¶å†™å…¥æ•°æ®æ—¶å‡ºé”™
data_lock = threading.Lock()

def is_english_content(text):
    """
    åˆ¤æ–­å†…å®¹æ˜¯å¦ä¸ºè‹±æ–‡ï¼ˆæ’é™¤ä¸­æ–‡ã€è¥¿ç­ç‰™è¯­ã€ä¿„è¯­ï¼‰
    ç®€å•åˆ¤æ–­ï¼šå¦‚æœåŒ…å«ä¸­æ–‡å­—ç¬¦ã€å¤§é‡è¥¿ç­ç‰™è¯­/ä¿„è¯­ç‰¹å¾å­—ç¬¦ï¼Œåˆ™è¿”å›False
    """
    if pd.isna(text) or not str(text).strip():
        return True  # ç©ºå†…å®¹é»˜è®¤ä¸ºè‹±æ–‡
    
    text = str(text)
    
    # æ£€æŸ¥ä¸­æ–‡å­—ç¬¦
    if re.search(r'[\u4e00-\u9fff]', text):
        return False
    
    # æ£€æŸ¥ä¿„è¯­å­—ç¬¦ï¼ˆè¥¿é‡Œå°”å­—æ¯ï¼‰
    if re.search(r'[Ğ-Ğ¯Ğ°-ÑĞÑ‘]', text):
        return False
    
    # æ£€æŸ¥è¥¿ç­ç‰™è¯­ç‰¹æ®Šå­—ç¬¦ç»„åˆï¼ˆå¦‚ Ã±, Ã¡, Ã©, Ã­, Ã³, Ãº ç­‰ï¼Œä½†è¿™äº›ä¹Ÿå¯èƒ½åœ¨è‹±æ–‡ä¸­å‡ºç°ï¼‰
    # æ›´å‡†ç¡®çš„æ–¹æ³•æ˜¯æ£€æŸ¥å¸¸è§çš„è¥¿ç­ç‰™è¯­è¯æ±‡
    spanish_words = ['mÃ¡quina', 'para', 'con', 'del', 'las', 'los', 'una', 'este', 'esta', 'serie']
    text_lower = text.lower()
    spanish_count = sum(1 for word in spanish_words if word in text_lower)
    # å¦‚æœåŒ…å«å¤šä¸ªè¥¿ç­ç‰™è¯­è¯æ±‡ï¼Œå¯èƒ½æ˜¯è¥¿ç­ç‰™è¯­
    if spanish_count >= 3:
        return False
    
    return True

def find_existing_image_by_id(product_id, images_dir="public/images/products"):
    """
    æ ¹æ®äº§å“IDæŸ¥æ‰¾å·²å­˜åœ¨çš„å›¾ç‰‡æ–‡ä»¶
    è¿”å›æ‰¾åˆ°çš„ç¬¬ä¸€ä¸ªå›¾ç‰‡è·¯å¾„ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›None
    """
    if not os.path.exists(images_dir):
        return None
    
    # å°è¯•å¤šç§å¯èƒ½çš„æ–‡ä»¶åæ¨¡å¼
    patterns = [
        f"*{product_id}*",
        f"*id-{product_id}*",
        f"*product-{product_id}*"
    ]
    
    for pattern in patterns:
        matches = glob.glob(os.path.join(images_dir, pattern))
        if matches:
            # è¿”å›ç¬¬ä¸€ä¸ªåŒ¹é…çš„æ–‡ä»¶ï¼Œè½¬æ¢ä¸ºwebè·¯å¾„
            rel_path = os.path.relpath(matches[0], "public")
            return f"/{rel_path.replace(os.sep, '/')}"
    
    return None

def download_image(url, save_name_prefix, index=0, product_id=None):
    """
    ä¸‹è½½å›¾ç‰‡ï¼Œå¦‚æœå·²å­˜åœ¨åˆ™ç›´æ¥è¿”å›è·¯å¾„
    ä¼˜å…ˆæ£€æŸ¥åŸºäºslugçš„æ–‡ä»¶ï¼Œå¦‚æœä¸å­˜åœ¨ï¼Œå°è¯•æ ¹æ®product_idæŸ¥æ‰¾
    """
    if pd.isna(url) or not str(url).strip():
        # å¦‚æœæ²¡æœ‰URLï¼Œå°è¯•æ ¹æ®product_idæŸ¥æ‰¾å·²å­˜åœ¨çš„å›¾ç‰‡
        if product_id and index == 0:  # åªå¯¹ä¸»å›¾å°è¯•
            existing = find_existing_image_by_id(product_id)
            if existing:
                return existing
        return None
    
    url = str(url).strip()
    if not url.lower().startswith(('http://', 'https://')):
        clean_domain = OLD_WEBSITE_DOMAIN.rstrip('/')
        clean_path = url.lstrip('/')
        full_url = f"{clean_domain}/{clean_path}"
    else:
        full_url = url

    ext = os.path.splitext(url.split('?')[0])[1]
    if not ext: ext = ".jpg"
    
    if index == 0:
        filename = f"{save_name_prefix}{ext}"
    else:
        filename = f"{save_name_prefix}-{index}{ext}"
        
    save_path = f"public/images/products/{filename}"
    os.makedirs('public/images/products', exist_ok=True)

    if os.path.exists(save_path):
        return f"/images/products/{filename}"

    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(full_url, headers=headers, timeout=10)
        if response.status_code == 200:
            with open(save_path, 'wb') as f:
                f.write(response.content)
            return f"/images/products/{filename}"
    except Exception:
        # å¦‚æœä¸‹è½½å¤±è´¥ï¼Œå°è¯•æ ¹æ®product_idæŸ¥æ‰¾å·²å­˜åœ¨çš„å›¾ç‰‡
        if product_id and index == 0:
            existing = find_existing_image_by_id(product_id)
            if existing:
                return existing
        return None
    return None

def extract_body_images(html_content, slug, product_id=None):
    """
    æå–è¯¦æƒ…å›¾ï¼Œå¦‚æœå›¾ç‰‡å·²å­˜åœ¨åˆ™ç›´æ¥ä½¿ç”¨
    """
    if pd.isna(html_content): return []
    soup = BeautifulSoup(str(html_content), 'html.parser')
    images = []
    img_tags = soup.find_all('img')
    for i, img in enumerate(img_tags, 1):
        src = img.get('src')
        if src:
            local_path = download_image(src, slug, index=i, product_id=product_id)
            if local_path:
                images.append(local_path)
    return images

def clean_html_text(html_content):
    if pd.isna(html_content): return ""
    return BeautifulSoup(str(html_content), 'html.parser').get_text(separator='\n').strip()

def process_single_product(row):
    """
    å¤„ç†å•ä¸ªäº§å“çš„æ ¸å¿ƒå‡½æ•°
    """
    try:
        # 0. è¯­è¨€è¿‡æ»¤ï¼šåªå¤„ç†è‹±æ–‡å†…å®¹
        title = str(row.get('title', ''))
        description = str(row.get('description', ''))
        body_text = clean_html_text(row.get('body', ''))
        
        # æ£€æŸ¥titleå’Œdescriptionæ˜¯å¦ä¸ºè‹±æ–‡
        if not is_english_content(title) or not is_english_content(description):
            return None  # è·³è¿‡éè‹±æ–‡äº§å“
        
        product_id = str(row.get('id', ''))
        
        # 1. AI å¤„ç†
        prompt = f"""
        Strictly Output JSON only. Rewrite for B2B SEO ({TARGET_LANGUAGE}).
        Title: {title}
        Body: {body_text[:1000]}
        
        Fields: productName, slug, seoTitle, metaDescription, features(array), specifications(object), fullContentHtml(tailwind css).
        """
        
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1
        )
        content = response.choices[0].message.content.strip().replace('```json', '').replace('```', '')
        ai_data = json.loads(content)
        
        slug = ai_data['slug']
        
        # 2. å›¾ç‰‡å¤„ç†ï¼ˆä¼ å…¥product_idä»¥ä¾¿æŸ¥æ‰¾å·²å­˜åœ¨çš„å›¾ç‰‡ï¼‰
        main_image = download_image(row.get('litpic'), slug, index=0, product_id=product_id)
        gallery_images = extract_body_images(row.get('body'), slug, product_id=product_id)
        
        # 3. è¿”å›ç»“æœ
        return {
            "id": product_id,
            **ai_data,
            "mainImage": main_image if main_image else "/images/placeholder.jpg",
            "gallery": gallery_images
        }

    except Exception as e:
        # å‡ºé”™æ—¶ä¸ä¸­æ–­ï¼Œåªæ‰“å°é”™è¯¯
        # print(f"Error processing {row.get('title')}: {e}")
        return None

def main():
    print(f"ğŸš€ å¯åŠ¨æé€Ÿæ¨¡å¼ (çº¿ç¨‹æ•°: {MAX_WORKERS})...")
    print(f"ğŸ“‹ å¤„ç†èŒƒå›´: ç´¢å¼• 0 åˆ° 851 (å…± 852 æ¡)")
    print(f"ğŸŒ è¯­è¨€è¿‡æ»¤: ä»…å¤„ç†è‹±æ–‡äº§å“")
    
    try:
        archives = pd.read_csv('dede_archives.csv', encoding='utf-8')
        addon = pd.read_csv('dede_addonshop.csv', encoding='utf-8')
    except:
        archives = pd.read_csv('dede_archives.csv', encoding='gbk')
        addon = pd.read_csv('dede_addonshop.csv', encoding='gbk')

    merged = pd.merge(archives, addon, left_on='id', right_on='aid', how='left')
    
    # ä»ç´¢å¼•0åˆ°851ï¼ˆå…±852æ¡ï¼‰
    merged = merged.iloc[0:851]
    
    # é¢„å…ˆè¿‡æ»¤ï¼šåªä¿ç•™è‹±æ–‡äº§å“ï¼ˆæé«˜æ•ˆç‡ï¼‰
    print("ğŸ” æ­£åœ¨è¿‡æ»¤è‹±æ–‡äº§å“...")
    english_mask = merged.apply(
        lambda row: is_english_content(row.get('title', '')) and 
                   is_english_content(row.get('description', '')),
        axis=1
    )
    merged = merged[english_mask]
    print(f"âœ… æ‰¾åˆ° {len(merged)} ä¸ªè‹±æ–‡äº§å“")
    
    # è½¬æ¢ä¸ºåˆ—è¡¨å­—å…¸ï¼Œæ–¹ä¾¿åˆ†é…ä»»åŠ¡
    all_tasks = merged.to_dict('records')
    
    final_results = []
    
    # ä½¿ç”¨ ThreadPoolExecutor å¹¶è¡Œå¤„ç†
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        futures = {executor.submit(process_single_product, task): task for task in all_tasks}
        
        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡
        for future in tqdm(as_completed(futures), total=len(all_tasks), desc="Processing"):
            result = future.result()
            if result:
                final_results.append(result)

    # ä¿å­˜
    print("ğŸ’¾ æ­£åœ¨ä¿å­˜æ•°æ®...")
    with open('products_final.json', 'w', encoding='utf-8') as f:
        json.dump(final_results, f, indent=2, ensure_ascii=False)
        
    print(f"\nâœ… å…¨éƒ¨å®Œæˆï¼æˆåŠŸå¤„ç† {len(final_results)} ä¸ªäº§å“ã€‚")

if __name__ == "__main__":
    main()