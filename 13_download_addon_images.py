import pandas as pd
import requests
import os
import time
from urllib.parse import urljoin
from tqdm import tqdm

# ================= ğŸ”§ é…ç½®åŒºåŸŸ =================
ADDON_CSV = "dede_addonshop.csv" 
BASE_URL = "https://www.fhopepack.com"
SAVE_DIR = "public/images/products"
# ===========================================

def download_image(url, save_path):
    """ä¸‹è½½å›¾ç‰‡ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    try:
        r = requests.get(url, headers=headers, timeout=15)
        if r.status_code == 200:
            with open(save_path, 'wb') as f:
                f.write(r.content)
            return True
        return False
    except Exception as e:
        return False

def main():
    print("Start downloading litpic1-4 images from dede_addonshop.csv...")
    
    # 1. å‡†å¤‡ç›®å½•
    if not os.path.exists(SAVE_DIR):
        os.makedirs(SAVE_DIR)

    # 2. è¯»å– CSV
    if not os.path.exists(ADDON_CSV):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {ADDON_CSV}")
        return

    try:
        df = pd.read_csv(ADDON_CSV, encoding='utf-8')
    except:
        print("âš ï¸ UTF-8 è¯»å–å¤±è´¥, å°è¯• GB18030...")
        df = pd.read_csv(ADDON_CSV, encoding='gb18030')

    # 3. æå–å¹¶å»é‡å›¾ç‰‡è·¯å¾„
    print("Extracting image links...")
    image_paths = set()
    
    target_cols = ['litpic1', 'litpic2', 'litpic3', 'litpic4']
    
    # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
    valid_cols = [col for col in target_cols if col in df.columns]
    
    if not valid_cols:
        print(f"Error: litpic1-4 columns not found in CSV")
        return

    for col in valid_cols:
        # è·å–è¯¥åˆ—æ‰€æœ‰éç©ºå€¼
        paths = df[col].dropna().astype(str).tolist()
        for p in paths:
            p = p.strip()
            if p and p.lower() != 'nan' and p != '':
                image_paths.add(p)

    print(f"Found {len(image_paths)} unique image links")

    downloaded_count = 0
    skipped_count = 0
    failed_count = 0

    # 4. éå†ä¸‹è½½
    for rel_path in tqdm(list(image_paths), desc="Processing images"):
        # æ¸…æ´—è·¯å¾„
        clean_path = rel_path
        if not clean_path.startswith('/'): 
            clean_path = '/' + clean_path
        
        # æå–æ–‡ä»¶å
        filename = os.path.basename(clean_path)
        # å»æ‰ URL å‚æ•° (?ver=1.0)
        filename = filename.split('?')[0] 
        
        # âš ï¸ ç®€å•çš„æ–‡ä»¶ååˆæ³•æ€§æ£€æŸ¥
        if not filename or len(filename) > 200:
            continue

        local_path = os.path.join(SAVE_DIR, filename)
        
        # === æ ¸å¿ƒé€»è¾‘ï¼šæ£€æŸ¥æ˜¯å¦å­˜åœ¨ ===
        if os.path.exists(local_path) and os.path.getsize(local_path) > 0:
            skipped_count += 1
            continue  # è·³è¿‡ï¼Œä¸éœ€è¦ä¸‹è½½
        
        # === å¦‚æœä¸å­˜åœ¨ï¼Œæˆ–è€…æ–‡ä»¶å¤§å°ä¸º0ï¼Œåˆ™ä¸‹è½½ ===
        full_url = urljoin(BASE_URL, clean_path)
        success = download_image(full_url, local_path)
        
        if success:
            downloaded_count += 1
            time.sleep(0.05) # ç¨å¾®ä¼‘æ¯ä¸‹é˜²æ­¢å°IP
        else:
            failed_count += 1
            # print(f"âŒ ä¸‹è½½å¤±è´¥: {full_url}")

    print("-" * 30)
    print(f"Task finished!")
    print(f"Skipped (already exists): {skipped_count}")
    print(f"Successfully downloaded: {downloaded_count}")
    print(f"Failed to download: {failed_count}")

if __name__ == "__main__":
    main()

